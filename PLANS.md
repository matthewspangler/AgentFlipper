# AgentFlipper AI Agent Loop Improvement Plan

This document outlines potential areas for improving the AI agent loop and tool calling mechanism in the AgentFlipper project, based on a comparison with other AI coding assistants like Cline, Roo Code, Aider, and Codename Goose.

## Current Agent Loop Structure Analysis

The current loop, primarily in `flipper-agent/request_processor.py`, operates as follows:
1. User input is received via the Textual UI.
2. A worker thread processes the input, calling `request_processor.py`.
3. The `request_processor` calls the LLM (`flipper-agent/llm_agent.py`) to get a list of tool calls.
4. The tool calls are executed sequentially within `request_processor.py`.
5. If the task is not marked complete by the LLM and the recursion depth limit is not reached, `process_user_request` calls itself recursively to handle subsequent steps.

## Comparison and Suggested Improvements

Here are key areas where the AgentFlipper loop differs from or could be enhanced based on patterns in other advanced AI agents:

### 1. Recursion vs. Iteration for Multi-Turn Tasks

*   **Current:** Uses a recursive function call to continue processing multi-step tasks, with tool calls generated by the LLM sequentially.
*   **Planned:** Adopt a Plan and Act with Self-Reflection pattern. The LLM will be explicitly prompted to generate a plan (a sequence of actions/tool calls). The agent will execute this plan. After execution (or a set of steps), the LLM will reflect on the results and decide the next course of action, potentially generating a new plan, adding more tasks, or signaling task completion. Refer to the diagram in `ARCHITECTURE.md` for a visual representation of this loop.
*   **Suggestion:** Refactor the current recursive process in `flipper-agent/request_processor.py` into an explicit iterative loop that implements this Plan and Act with Self-Reflection cycle. This will improve clarity, control flow, and robustness.

### 2. Tool Execution Management

*   **Current:** Tool execution logic is directly embedded within the `if/elif` blocks in `flipper-agent/request_processor.py`.
*   **Comparison:** Many well-designed agent architectures separate the LLM's decision-making (generating tool calls) from the system's execution of those tools. They use a dedicated "Tool Dispatcher" or "Tool Executor" component.
*   **Suggestion:** Create a separate function or class responsible for receiving the parsed tool calls from `llm_agent.py` and invoking the corresponding functions in `FlipperZeroManager` or updating the UI via `app_instance.display_message`. This centralizes tool execution and improves modularity.

### 3. State Management

*   **Current:** Conversation history and some task state reside within the `LLMAgent`. The `app_instance` is passed to various functions for UI interaction.
*   **Comparison:** Centralized state management can simplify complex agent architectures.
*   **Suggestion:** Consider introducing a dedicated `AgentState` object or a centralized state management pattern. This object could hold the conversation history, task status, and references to key components like `FlipperZeroManager` and `LLMAgent`, reducing the need to pass multiple parameters between functions.

### 4. Error Handling and Reporting

*   **Current:** Errors during command execution are caught, logged, and an error message is displayed. Results, including errors, are added to the conversation history for the LLM.
*   **Comparison:** More advanced agents may use structured error reporting back to the LLM or implement specific retry/fallback strategies for tools.
*   **Suggestion:** While your current logging and history inclusion are helpful, consider if a more explicit and structured way of reporting tool failures (e.g., a specific "tool_result" message type indicating an error) could improve the LLM's ability to diagnose and recover from issues.

### 5. Tool Definition and Extensibility

*   **Current:** Tools are defined as a hardcoded list of dictionaries in `flipper-agent/llm_agent.py`.
*   **Comparison:** Some frameworks offer more abstract or configuration-driven ways to define tools, which can simplify adding new capabilities.
*   **Suggestion:** If you plan to add many more tools, explore patterns for defining them outside the core `LLMAgent` logic, perhaps using a configuration file or a dedicated tool registry.

### 6. Observability

*   **Current:** Standard Python logging is used effectively.
*   **Comparison:** Advanced agents often include detailed tracing of the agent's internal steps (planning, tool selection, execution, observation) to provide deeper insight into behavior.
*   **Suggestion:** As the agent grows, consider adding more granular logging or a simple tracing mechanism to better understand the flow of control and data through the loop.

Implementing these suggestions can help evolve AgentFlipper towards a more robust, flexible, and scalable AI agent architecture.
## Task Management Enhancements

Based on the discussion about handling multiple ordered subtasks:

*   **Data Structure:** Implement task and subtask management using nested Python lists, where lists contain action items (potentially dictionaries) and some action items can themselves contain nested lists of sub-actions. This allows for dynamic and hierarchical task structures.
*   **UI Visualization:** Develop a filetree-like visual feedback element in the user interface (using Textual) to display the current task queue structure and highlight the currently executing task/subtask. This will provide users with clear insight into the agent's progress through complex plans.