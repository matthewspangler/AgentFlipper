"""
Processes user requests and interacts with the Flipper Zero and LLM agents.
"""

import logging
from typing import Optional, List, Tuple

from hardware_manager import FlipperZeroManager
from llm_agent import LLMAgent

logger = logging.getLogger("FlipperAgent")

async def process_user_request(app_instance, user_input: str, flipper_agent: FlipperZeroManager, llm_agent: LLMAgent, recursion_depth: int = 0):
    """
    Process a user request and execute any tool calls.
    Args:
        app_instance: The TextualApp instance for UI updates.
        user_input: The user's input text
        flipper_agent: The agent that communicates with Flipper Zero
        llm_agent: The agent that generates tool calls using LLM
        recursion_depth: Current recursion depth for follow-up tool calls

    Returns:
        None
    """
    # Guard against excessive recursion
    if recursion_depth >= llm_agent.max_recursion_depth:
        logger.warning(f"Maximum recursion depth reached ({llm_agent.max_recursion_depth}), stopping chain")
        await app_instance.display_message(f"\n[red]Maximum loop depth reached ({llm_agent.max_recursion_depth}). Please issue a new command to continue.[/]")
        # Generate a final summary when we stop due to recursion limit
        summary = await llm_agent.generate_summary() # Await the async summary generation
        await app_instance.display_message(f"\n[cyan]Final Summary:[/]")
        await app_instance.display_message(f"{summary}\n")
        return

    # Generate summary every 3 iterations to keep context fresh
    if recursion_depth > 0 and recursion_depth % 3 == 0:
        summary = await llm_agent.generate_summary() # Await the async summary generation
        await app_instance.display_message(f"\n[cyan]Progress Summary:[/]")
        await app_instance.display_message(f"{summary}\n")

    # Get tool calls from LLM
    tool_calls = await llm_agent.get_commands(user_input) # Await the async LLM call

    # Handle empty response
    if not tool_calls:
        logger.warning("No tool calls were generated by the LLM")
        await app_instance.display_message(f"\n[yellow]No response was generated. Try rephrasing your request.[/]")
        return

    # Process each tool call
    for call in tool_calls:
        name = call["name"]
        args = call["arguments"]

        if name == "execute_commands":
            commands = args["commands"]
            # Execute commands and get results
            await app_instance.display_message(f"[green]{'─' * 50}[/]")
            await app_instance.display_message(f"[purple]Executing {len(commands)} commands...[/]")
            for i, cmd in enumerate(commands, 1):
                await app_instance.display_message(f"[bold]{i}.[/] {cmd}")
            await app_instance.display_message(f"[green]{'─' * 50}[/]")
            # Pass the app_instance to execute_commands for direct UI updates
            results = await flipper_agent.execute_commands(commands, app_instance) # Await the async Flipper call, pass app_instance

            # Check for command errors
            command_failed = False
            for cmd, resp in results:
                if "error" in resp.lower() or "illegal" in resp.lower():
                    command_failed = True
                    break

            # Add results to conversation history for better summaries
            llm_agent.add_execution_results_to_history(results) # This method is synchronous and does not need await

            # If command failed, provide error information
            if command_failed:
                error_info = "Command execution failed. Please check the device response."
                await app_instance.display_message(f"[red]{'─' * 50}[/]")
                await app_instance.display_message(f"[red bold]# Error:[/]")
                await app_instance.display_message(f"[bold red]{error_info}[/]")
                await app_instance.display_message(f"[red]{'─' * 50}[/]")

        elif name == "provide_information":
            information = args["information"]
            # Display information to user
            await app_instance.display_message(f"[green]{'─' * 50}[/]")
            await app_instance.display_message(f"[green]# Information:[/]")
            await app_instance.display_message(f"[bold]{information}[/]")
            await app_instance.display_message(f"[green]{'─' * 50}[/]")

        elif name == "ask_question":
            question = args["question"]
            # Ask user question - this will need further Textual integration
            await app_instance.display_message(f"[blue]{'─' * 50}[/]")
            await app_instance.display_message(f"[blue]? {question}[/]")
            await app_instance.display_message(f"[blue]{'─' * 50}[/]")
            # Returning here pauses processing for user input in the original UI.
            # In Textual, handling user input is asynchronous. This part needs
            # significant refactoring to work with Textual's input handling.
            # For now, we'll just display the question.
            return  # Cannot pause execution synchronously in Textual worker

        elif name == "mark_task_complete":
            # Mark task as complete
            await app_instance.display_message(f"[green]{'─' * 50}[/]")
            await app_instance.display_message(f"[green]✓ Task completed [/]")
            await app_instance.display_message(f"[green]{'─' * 50}[/]")
            llm_agent.task_in_progress = False
            llm_agent.task_description = ""
            return True  # Indicate task completion

    # If we get here, we processed all tool calls without completing the task or asking a question
    # Check if we executed any commands - if so, force task completion to prevent infinite loops
    command_executed = any(call["name"] == "execute_commands" for call in tool_calls)
    task_completed = any(call["name"] == "mark_task_complete" for call in tool_calls)

    if command_executed and not task_completed and recursion_depth > 0:
        await app_instance.display_message(f"\n[orange]Command executed but task not marked complete. Forcing completion to prevent loop.[/]")
        llm_agent.task_in_progress = False
        llm_agent.task_description = ""
        return True

    # Continue the loop if under recursion limit
    if recursion_depth < llm_agent.max_recursion_depth:
        await app_instance.display_message(f"\n[purple]Continuing task...[/]")
        # Pass the app_instance for recursive calls and await the async function
        await process_user_request(app_instance, user_input, flipper_agent, llm_agent, recursion_depth + 1)